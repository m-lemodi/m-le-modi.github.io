[{"content":"Stack HashiCorp  HashiCorp est une entreprise de software base à San Francisco en Californie. Cette entreprise propose une suite de logiciels open-source orientés service. Deux d\u0026rsquo;entre eux sont Consul et Nomad. \u0026raquo; plus d\u0026rsquo;info\n Consul Consul est un soft permettant de faire du service discovery. C\u0026rsquo;est à dire faire l\u0026rsquo;inventaire des services disponibles dans un cluster et récupérer les informations (up, down, \u0026hellip;). Il est également possible de set des key values qui seront réutilisées par les services (comme Nomad par exemple).\nHello World d\u0026rsquo;un agent Consul (avec le client ayant l\u0026rsquo;ip de l\u0026rsquo;interface publique) :\n$ consul agent -dev -client '{{ GetInterfaceIP \u0026quot;ens18\u0026quot; }}' Lorsque l\u0026rsquo;on se rend sur XX.XX.XX.XX:5500, on peut accéder au GUI de Consul :\nDans notre cas, on ne voit que consul comme service car on a encore rien configuré comme service.\nPour faire de la HA (cf cours haute disponibilité), on met plusieurs serveurs Consul (on utilise pas corosync mais ça fonctionne pareil). Si on change une Key Value sur un serveur, les autres se mettent à jour automatiquement.\nNomad Nomad est un orchestrateur. C\u0026rsquo;est à dire qu\u0026rsquo;il va gérer les services : si on lui demande n services up, il va s\u0026rsquo;arranger pour avoir toujours ces n services up. Lorsqu\u0026rsquo;on va deploy une maj, il va down, mettre à jour et up les services chacun leur tour. Il va effectuer aussi des check que l\u0026rsquo;on aura configuré pour savoir si notre service est healthy ou non\n Nomad va parler à Consul et lui dire \u0026ldquo;j\u0026rsquo;ai tels services qui marchent, voici leur IP, leur port, \u0026hellip;\u0026rdquo;.\n Admettons que nous ayons une image Docker simple contenant un nginx et un index.html. Nous voulons 3 serveurs qui tournent et afficher ces serveurs en tant que service dans Consul. Voici un exemple de configuration :\n web.nomad  job \u0026quot;web\u0026quot; { datacenters = [\u0026quot;dc1\u0026quot;] type = \u0026quot;service\u0026quot; update { max_parallel = 1 min_healthy_time = \u0026quot;10s\u0026quot; healthy_deadline = \u0026quot;3m\u0026quot; progress_deadline = \u0026quot;10m\u0026quot; auto_revert = false canary = 0 } migrate { max_parallel = 1 health_check = \u0026quot;checks\u0026quot; min_healthy_time = \u0026quot;10s\u0026quot; healthy_deadline = \u0026quot;5m\u0026quot; } group \u0026quot;cache\u0026quot; { # nombre de serveurs à lancer count = 3 # port du serv nginx dans le container network { port \u0026quot;serv\u0026quot; { to = 80 } } service { name = \u0026quot;web-cache\u0026quot; tags = [\u0026quot;global\u0026quot;, \u0026quot;cache\u0026quot;] port = \u0026quot;serv\u0026quot; # check à faire pour vérifier que le serv est healthy. ici on check la co tcp check { type = \u0026quot;tcp\u0026quot; interval = \u0026quot;10s\u0026quot; timeout = \u0026quot;2s\u0026quot; } } restart { attempts = 2 interval = \u0026quot;30m\u0026quot; delay = \u0026quot;15s\u0026quot; mode = \u0026quot;fail\u0026quot; } ephemeral_disk { size = 300 } # lancement d'un container task \u0026quot;nginx\u0026quot; { driver = \u0026quot;docker\u0026quot; config { # image docker citée plus haut image = \u0026quot;web-1:v1\u0026quot; ports = [\u0026quot;serv\u0026quot;] } resources { cpu = 500 # 500 MHz memory = 256 # 256MB } } } }  server.conf  data_dir = \u0026quot;/var/lib/nomad\u0026quot; bind_addr = \u0026quot;0.0.0.0\u0026quot; advertise { # adresse publique http = \u0026quot;XX.XX.XX.XX\u0026quot; } server { enabled = true bootstrap_expect = 1 } client { enabled = true # interface publique network_interface = \u0026quot;ens18\u0026quot; } consul { # adresse où se trouve consul address = \u0026quot;XX.XX.XX.XX:8500\u0026quot; } (XX.XX.XX.XX fait référence à l\u0026rsquo;IP publique)\nIl ne reste plus qu\u0026rsquo;à lancer l\u0026rsquo;agent nomad, de la même manière que Consul, et de lancer le job que nous avons configuré.\n$ nomad agent -dev-connect -config=server.conf -log-level INFO $ nomad job run web.nomad On obtient bien nos 3 serveurs nginx qui tournent :\n$ nomad status ID Type Priority Status Submit Date web service 50 running 2021-05-25T11:00:38+02:00 $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 0a60baada3f7 web-1:v1 \u0026quot;/docker-entrypoint.…\u0026quot; 3 hours ago Up 3 hours XX.XX.XX.XX:28546-\u0026gt;80/tcp, XX.XX.XX.XX:28546-\u0026gt;80/udp nginx-6a9237b6-eae4-22e0-fbf9-b643388af041 876db102ec67 web-1:v1 \u0026quot;/docker-entrypoint.…\u0026quot; 3 hours ago Up 3 hours XX.XX.XX.XX:27303-\u0026gt;80/tcp, XX.XX.XX.XX:27303-\u0026gt;80/udp nginx-e914eb29-dd02-afe1-c8b5-10bfcd677aa5 032903bba6c2 web-1:v1 \u0026quot;/docker-entrypoint.…\u0026quot; 3 hours ago Up 3 hours XX.XX.XX.XX:22854-\u0026gt;80/tcp, XX.XX.XX.XX:22854-\u0026gt;80/udp nginx-3ef4fd44-a394-958a-0db7-84f8fdbb554c Si tout s\u0026rsquo;est bien passé, on obtient alors nos services dans l\u0026rsquo;interface de Consul :\nIntégration de HAProxy pour faire du load balancing Maintenant que nos services sont enregistrés dans Consul, on peut les récupérer avec HAProxy pour faire du LB. Configurons notre HAProxy de la sorte :\n /etc/haproxy/haproxy.cfg  global # Config de base que je n'ai pas modifié. Voir la doc pour plus d'info log /dev/log\tlocal0 log /dev/log\tlocal1 notice chroot /var/lib/haproxy stats socket /run/haproxy/admin.sock mode 660 level admin expose-fd listeners stats timeout 30s user haproxy group haproxy daemon # Default SSL material locations ca-base /etc/ssl/certs crt-base /etc/ssl/private defaults log\tglobal mode\thttp option\thttplog option\tdontlognull timeout connect 5000 timeout client 50000 timeout server 50000 errorfile 400 /etc/haproxy/errors/400.http errorfile 403 /etc/haproxy/errors/403.http errorfile 408 /etc/haproxy/errors/408.http errorfile 500 /etc/haproxy/errors/500.http errorfile 502 /etc/haproxy/errors/502.http errorfile 503 /etc/haproxy/errors/503.http errorfile 504 /etc/haproxy/errors/504.http frontend stats # on met en place un front qui affichera les stats de HAProxy sur le port 1936 (voir la doc) bind :1936 stats uri / stats show-legends no log frontend www # notre frontend sera sur le port 8889 et aura la backend www_back bind :8889 default_backend www_back backend www_back # on utilise l'algo roundrobin pour le LB balance roundrobin # on utilise une template de serveur qu'on pioche directement dans consul # la syntaxe est _SERVICE._TAG.service.consul server-template web 3 _web-cache._cache.service.consul resolvers consul resolve-opts allow-dup-ip resolve-prefer ipv4 check # ces lignes sont optionnelles, c'est pour que j'affiche dans le response header l'ip et le nom du backend utilisé http-response set-header backend-ip %si:%sp http-response set-header backend-name %s resolvers consul # l'adresse du serveur consul. Attention le port est 8600 nameserver consul 10.41.7.129:8600 accepted_payload_size 8192 hold valid 5s On reload notre HAProxy\n$ systemctl reload haproxy Si tout s\u0026rsquo;est bien passé on a sur XX.XX.XX.XX:8889 un de nos serveur nginx. On peut savoir lequel en regardant le response head (F12). De plus, on peut avoir une vue de nos serveurs sur le lien des stats (XX.XX.XX.XX:1936 que nous avons config) :\nEt voilà notre petite infra est terminée ! that\u0026rsquo;s all folks\n","date":"2021-05-25T00:00:00Z","image":"https://borisjacquot.github.io/p/stack-hashicorp/hashicorp_hud48d475653a68121c5c5dd4c9c15f623_315161_120x120_fill_q75_box_smart1.jpg","permalink":"https://borisjacquot.github.io/p/stack-hashicorp/","title":"Stack HashiCorp"},{"content":"La haute disponibilité Intro Le but d\u0026rsquo;une HA est de rendre un service disponible quand un serveur est surchargé. Dans notre cas nous prendrons l\u0026rsquo;exemple d\u0026rsquo;un site web static dans un premier temps.\nTODO: schéma\nNous avons notre site qui tourne sur un serveur apache, jusque ici tout va bien. Admettons que le serveur soit surchargé, comment assurer la disponibilité du site ?\n-\u0026gt; on va dans un premier temps multiplier le nombre de serveurs qui font tourner le site.\nMais comment faire quand tous les serveurs sont surchargés ?\nLes load balencers (LB) Il existe plusieures techno de LB :\n Apache nginx HAProxy traefic  A quoi ça sert ?\nUn LB va distribuer le traffic vers les serveurs. Si un serveur tombe en panne, il va le retirer de son pool et l\u0026rsquo;accès au service sera toujours possible.\nCependant, un seul LB crée un SPOF (point de défaillance unique), c\u0026rsquo;est à dire que si il tombe, tout tombe (comme DC3 lol). Pour éviter ceci, on a la possibilité de mettre plusieurs LB. Mais comment choisir quel LB il faut utiliser ?\n-\u0026gt; On utilise le système de Quorum\n On va mettre en place un nombre impair de LB afin qu\u0026rsquo;il n\u0026rsquo;y ait pas de \u0026ldquo;vote nul\u0026rdquo;\n On va les mettre en place grâce à deux techno : corosync et pacemaker\n  Corosync va créer un cluster où les LB discutent en paires. Un LB sera désigné comme étant le maître et les autres des esclaves. Si le maître vient à mourir, corosync va transmettre l\u0026rsquo;information aux autres LB et ces derniers vont élire un nouveau maître\n  Pacemaker partage la ressource (ici l\u0026rsquo;IP). La ressource doit toujours être sur le maître. Quand le maître décède, Pacemaker va informer Corosync qu\u0026rsquo;il n\u0026rsquo;est plus apte à être maître.\n  Passage en dynamique Si maintenant notre site passe en dynamique, avec une BDD par exemple, si nous mettons simplement une BDD liée à l\u0026rsquo;API, nous nous confrontons à un SPOF à ce niveau.\nschema\nLa solution est donc de mettre une fois de plus des LB, ainsi que plusieurs BDD.\n  Les LB fonctionnent de la même manière. Les BDD fonctionnent en maître-esclave + une n+1 ième sert de backup.   Backbone todo + schema\nLes CDN Pour être ncore plus efficace, on peut mettre un CDN avant les LB. Un CDN permet de mettre en cache des réponses à des requêtes pour éviter de passer par tout le processus vu précédemment afin de perdre le moins de temps possible. Une des technos utilisées est Varnish\nschema maybe\n Cloudflare sert, entre autre, de cache.\n ","date":"2021-05-22T00:00:00Z","image":"https://borisjacquot.github.io/p/la-haute-disponibilit%C3%A9/haute-disponibilite_hu3d03a01dcc18bc5be0e67db3d8d209a6_2329900_120x120_fill_q75_box_smart1.jpg","permalink":"https://borisjacquot.github.io/p/la-haute-disponibilit%C3%A9/","title":"La haute disponibilité"},{"content":"Les conteneurs Random things ps et pstree permettent de voir la liste des processus actifs sur la machine (package procps). Au démarrage le processus 1 sera toujours /init\n Dans le cas d\u0026rsquo;une machine virtuelle, l\u0026rsquo;information va être traitée une première fois par la VM, puis est réinterprêtée par le noyau. (On peut skip ceci et gagner du temps avec VT-x)\n Un conteneur =/= une VM\nLes Namespaces   Processus : Les prcessus dans le NS vont être isolés des autres. À l\u0026rsquo;intérieur du NS, le premier processus aura donc l\u0026rsquo;ID 1 (cf /init). Du point de vue du noyau, ce processus aura un autre ID qui lui est propre. Il est impossible, à l\u0026rsquo;intérieur du NS, de voir les autres processus de la machine. Cependant, l\u0026rsquo;inverse est vrai. (cf pstree dans chaque)\n  Réseau : On peut créer une interface virtuelle qui sera alors connectée (ou non) à une interface de la machine (via un bridge par exemple).\n  User / group : Les utilisateurs dans un NS peuvent avoir le même ID que ceux de la machine. Il peut donc y avoir des faille (par exemple : avoir l\u0026rsquo;ID 0 dans le NS donne les droits du l\u0026rsquo;ID 0 de la machine = root). Pour y remédier, on utilise l\u0026rsquo;UID shifting =\u0026gt; càd on va dire que tous les UID seront +4000 (par exemple) par rapport à ceux de la machine, donc plus sécurisant.\n  Docker  Un ensemble de NS crée un conteneur. On le crée grâce à un clone() (fork ou thread).\n Docker va simplifier la création de conteneurs en s\u0026rsquo;occupant de créer tous les NS. On peut choisir ce qu\u0026rsquo;on met dedans grâce à une image.\nPar exemple : pour un site web static qui tourne sous nginx, on va procéder de la sorte :\n On va créer un fichier Dockerfile avec toutes les instructions pour installer le conteneur (comment build l\u0026rsquo;image). Dans notre cas on pourra écrire dedans :  # on dl l'image que l'on souhaite =\u0026gt; ici nginx qui tourne sur alpine (une distro avec le minimum légal pour que ce soit léger) FROM nginx:alpine # on lance des commandes pour le build RUN apt update \u0026amp;\u0026amp; apt install [...] # on copie notre index (par exemple) COPY ./docker/base/fpm/resources/index.html /usr/local/nginx/html # on copie notre script qui sera le proc 1 selon ce qu'on veut COPY ./docker/base/fpm/resources/index.html /entrypoint # on défini un user pour exécuter les choses =\u0026gt; on donne peu de droits pour éviter les failles USER www-data # on choisi l'espace de travail WORKDIR \u0026quot;/application\u0026quot; # on choisi notre point d'entrée (donc le proc 0) ENTRYPOINT [\u0026quot;/entrypoint\u0026quot;]  Ensuite on peut lancer :  $ docker build -t my-image:v0.1 . et hop!\n","date":"2021-05-22T00:00:00Z","image":"https://borisjacquot.github.io/p/les-conteneurs/conteneurs_hu3d03a01dcc18bc5be0e67db3d8d209a6_3399148_120x120_fill_q75_box_smart1.jpg","permalink":"https://borisjacquot.github.io/p/les-conteneurs/","title":"Les conteneurs"}]